{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ed8d71-04a3-4839-b9a5-0624253f05f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 11:27:49,551 - INFO - Start Training on Domain 0...\n",
      "100% 1379/1379 [00:26<00:00, 52.56batch/s]\n",
      "2024-03-28 11:28:15,791 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.1498748452299321\n",
      "100% 1379/1379 [00:26<00:00, 52.75batch/s]\n",
      "2024-03-28 11:28:41,937 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.07539469600527496\n",
      "100% 1379/1379 [00:26<00:00, 52.44batch/s]\n",
      "2024-03-28 11:29:08,237 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.050490042698768375\n",
      "2024-03-28 11:29:08,403 - INFO - Start Training on Domain 1...\n",
      "100% 1544/1544 [00:28<00:00, 53.90batch/s]\n",
      "2024-03-28 11:29:37,052 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.02717698731464503\n",
      "100% 1544/1544 [00:29<00:00, 52.90batch/s]\n",
      "2024-03-28 11:30:06,243 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.014271405028478469\n",
      "100% 1544/1544 [00:28<00:00, 53.27batch/s]\n",
      "2024-03-28 11:30:35,230 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.01051456035743749\n",
      "2024-03-28 11:30:35,411 - INFO - Start Training on Domain 2...\n",
      "100% 1593/1593 [00:29<00:00, 54.33batch/s]\n",
      "2024-03-28 11:31:04,734 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0006790454794663868\n",
      "100% 1593/1593 [00:29<00:00, 54.35batch/s]\n",
      "2024-03-28 11:31:34,051 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000644434919862611\n",
      "100% 1593/1593 [00:29<00:00, 54.48batch/s]\n",
      "2024-03-28 11:32:03,293 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000501276179602466\n",
      "2024-03-28 11:32:03,478 - INFO - Start Training on Domain 3...\n",
      "100% 1614/1614 [00:30<00:00, 53.74batch/s]\n",
      "2024-03-28 11:32:33,517 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003627685707667208\n",
      "100% 1614/1614 [00:29<00:00, 54.89batch/s]\n",
      "2024-03-28 11:33:02,927 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00029837416053335317\n",
      "100% 1614/1614 [00:30<00:00, 53.56batch/s]\n",
      "2024-03-28 11:33:33,064 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.000252549756950753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 343.6957540512085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1221/1221 [00:05<00:00, 204.29batch/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_tse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_sse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/tse_sse_2022.csv\", index = False)\n",
    "    out_data = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f88e034-d5aa-47ab-8a80-b33155650a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 11:53:18,989 - INFO - Start Training on Domain 0...\n",
      "100% 1379/1379 [03:50<00:00,  5.97batch/s]\n",
      "2024-03-28 11:57:09,857 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.18841611560733965\n",
      "100% 1379/1379 [03:57<00:00,  5.79batch/s]\n",
      "2024-03-28 12:01:07,840 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.09488462933971992\n",
      "100% 1379/1379 [03:56<00:00,  5.83batch/s]\n",
      "2024-03-28 12:05:04,502 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.0635881830022146\n",
      "2024-03-28 12:05:04,596 - INFO - Start Training on Domain 1...\n",
      "100% 1544/1544 [04:23<00:00,  5.86batch/s]\n",
      "2024-03-28 12:09:28,234 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0005071090831098007\n",
      "100% 1544/1544 [04:24<00:00,  5.84batch/s]\n",
      "2024-03-28 12:13:52,478 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0006724774561233703\n",
      "100% 1544/1544 [04:23<00:00,  5.86batch/s]\n",
      "2024-03-28 12:18:15,857 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.0005877116053254185\n",
      "2024-03-28 12:18:15,954 - INFO - Start Training on Domain 2...\n",
      "100% 1593/1593 [04:33<00:00,  5.83batch/s]\n",
      "2024-03-28 12:22:49,101 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0010246005872754428\n",
      "100% 1593/1593 [04:33<00:00,  5.83batch/s]\n",
      "2024-03-28 12:27:22,122 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0007276599934442718\n",
      "100% 1593/1593 [04:33<00:00,  5.83batch/s]\n",
      "2024-03-28 12:31:55,484 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0005005989208128393\n",
      "2024-03-28 12:31:55,589 - INFO - Start Training on Domain 3...\n",
      "100% 1614/1614 [04:36<00:00,  5.84batch/s]\n",
      "2024-03-28 12:36:31,959 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.00016220550074748686\n",
      "100% 1614/1614 [04:36<00:00,  5.84batch/s]\n",
      "2024-03-28 12:41:08,301 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00020138175968805505\n",
      "100% 1614/1614 [04:35<00:00,  5.86batch/s]\n",
      "2024-03-28 12:45:43,748 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00020500237544562294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3144.8468046188354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 3592/3592 [01:13<00:00, 49.08batch/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_tse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_tse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/tse_tse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ec8298-4cd9-48bb-bae9-80e3bb5faf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 12:48:03,654 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 12:48:03,654 - INFO - Start Training on Domain 0...\n",
      "100% 1379/1379 [03:46<00:00,  6.08batch/s]\n",
      "2024-03-28 12:51:50,476 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.21872177894245443\n",
      "2024-03-28 12:51:50,476 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.21872177894245443\n",
      "100% 1379/1379 [03:53<00:00,  5.91batch/s]\n",
      "2024-03-28 12:55:43,791 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10983961416331428\n",
      "2024-03-28 12:55:43,791 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10983961416331428\n",
      "100% 1379/1379 [03:53<00:00,  5.90batch/s]\n",
      "2024-03-28 12:59:37,327 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07349410247897825\n",
      "2024-03-28 12:59:37,327 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07349410247897825\n",
      "2024-03-28 12:59:37,438 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 12:59:37,438 - INFO - Start Training on Domain 1...\n",
      "100% 1544/1544 [04:21<00:00,  5.91batch/s]\n",
      "2024-03-28 13:03:58,497 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.002469824456339557\n",
      "2024-03-28 13:03:58,497 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.002469824456339557\n",
      "100% 1544/1544 [04:22<00:00,  5.88batch/s]\n",
      "2024-03-28 13:08:21,027 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.008504616160855334\n",
      "2024-03-28 13:08:21,027 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.008504616160855334\n",
      "100% 1544/1544 [04:21<00:00,  5.90batch/s]\n",
      "2024-03-28 13:12:42,623 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.005882346254873165\n",
      "2024-03-28 13:12:42,623 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.005882346254873165\n",
      "2024-03-28 13:12:42,713 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 13:12:42,713 - INFO - Start Training on Domain 2...\n",
      "100% 1593/1593 [04:29<00:00,  5.91batch/s]\n",
      "2024-03-28 13:17:12,211 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0008606228542473805\n",
      "2024-03-28 13:17:12,211 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0008606228542473805\n",
      "100% 1593/1593 [04:29<00:00,  5.91batch/s]\n",
      "2024-03-28 13:21:41,575 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0006224913949384229\n",
      "2024-03-28 13:21:41,575 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0006224913949384229\n",
      "100% 1593/1593 [04:29<00:00,  5.92batch/s]\n",
      "2024-03-28 13:26:10,879 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0005389429834640332\n",
      "2024-03-28 13:26:10,879 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0005389429834640332\n",
      "2024-03-28 13:26:10,971 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 13:26:10,971 - INFO - Start Training on Domain 3...\n",
      "100% 1614/1614 [04:32<00:00,  5.91batch/s]\n",
      "2024-03-28 13:30:43,924 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001729766767855175\n",
      "2024-03-28 13:30:43,924 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001729766767855175\n",
      "100% 1614/1614 [04:32<00:00,  5.92batch/s]\n",
      "2024-03-28 13:35:16,683 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00024000657872381116\n",
      "2024-03-28 13:35:16,683 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00024000657872381116\n",
      "100% 1614/1614 [04:32<00:00,  5.93batch/s]\n",
      "2024-03-28 13:39:48,910 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0002656895090286912\n",
      "2024-03-28 13:39:48,910 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0002656895090286912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3105.3434252738953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1599/1599 [00:28<00:00, 55.79batch/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_tse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_szse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/tse_szse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04741b3-f0e0-40aa-b3bf-7c9bdf315a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 13:41:34,741 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 13:41:34,741 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 13:41:34,741 - INFO - Start Training on Domain 0...\n",
      "100% 1379/1379 [03:51<00:00,  5.97batch/s]\n",
      "2024-03-28 13:45:25,919 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.20027468511409263\n",
      "2024-03-28 13:45:25,919 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.20027468511409263\n",
      "2024-03-28 13:45:25,919 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.20027468511409263\n",
      "100% 1379/1379 [03:59<00:00,  5.75batch/s]\n",
      "2024-03-28 13:49:25,674 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10056276772167207\n",
      "2024-03-28 13:49:25,674 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10056276772167207\n",
      "2024-03-28 13:49:25,674 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10056276772167207\n",
      "100% 1379/1379 [03:59<00:00,  5.76batch/s]\n",
      "2024-03-28 13:53:24,948 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.06723204988838574\n",
      "2024-03-28 13:53:24,948 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.06723204988838574\n",
      "2024-03-28 13:53:24,948 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.06723204988838574\n",
      "2024-03-28 13:53:25,052 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 13:53:25,052 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 13:53:25,052 - INFO - Start Training on Domain 1...\n",
      "100% 1544/1544 [04:26<00:00,  5.80batch/s]\n",
      "2024-03-28 13:57:51,427 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.005959467453293387\n",
      "2024-03-28 13:57:51,427 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.005959467453293387\n",
      "2024-03-28 13:57:51,427 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.005959467453293387\n",
      "100% 1544/1544 [04:30<00:00,  5.71batch/s]\n",
      "2024-03-28 14:02:21,796 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0031543697865071847\n",
      "2024-03-28 14:02:21,796 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0031543697865071847\n",
      "2024-03-28 14:02:21,796 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0031543697865071847\n",
      "100% 1544/1544 [04:28<00:00,  5.75batch/s]\n",
      "2024-03-28 14:06:50,470 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002177019028108895\n",
      "2024-03-28 14:06:50,470 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002177019028108895\n",
      "2024-03-28 14:06:50,470 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002177019028108895\n",
      "2024-03-28 14:06:50,573 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 14:06:50,573 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 14:06:50,573 - INFO - Start Training on Domain 2...\n",
      "100% 1593/1593 [04:37<00:00,  5.75batch/s]\n",
      "2024-03-28 14:11:27,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0007842574116574543\n",
      "2024-03-28 14:11:27,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0007842574116574543\n",
      "2024-03-28 14:11:27,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0007842574116574543\n",
      "100% 1593/1593 [04:37<00:00,  5.74batch/s]\n",
      "2024-03-28 14:16:05,385 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000552296884035211\n",
      "2024-03-28 14:16:05,385 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000552296884035211\n",
      "2024-03-28 14:16:05,385 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000552296884035211\n",
      "100% 1593/1593 [04:37<00:00,  5.73batch/s]\n",
      "2024-03-28 14:20:43,213 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004082206464394986\n",
      "2024-03-28 14:20:43,213 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004082206464394986\n",
      "2024-03-28 14:20:43,213 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004082206464394986\n",
      "2024-03-28 14:20:43,301 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 14:20:43,301 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 14:20:43,301 - INFO - Start Training on Domain 3...\n",
      "100% 1614/1614 [04:40<00:00,  5.75batch/s]\n",
      "2024-03-28 14:25:23,870 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005951841030606846\n",
      "2024-03-28 14:25:23,870 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005951841030606846\n",
      "2024-03-28 14:25:23,870 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005951841030606846\n",
      "100% 1614/1614 [04:43<00:00,  5.70batch/s]\n",
      "2024-03-28 14:30:06,980 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0003546652749937854\n",
      "2024-03-28 14:30:06,980 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0003546652749937854\n",
      "2024-03-28 14:30:06,980 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0003546652749937854\n",
      "100% 1614/1614 [04:40<00:00,  5.75batch/s]\n",
      "2024-03-28 14:34:47,796 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00026643120955702\n",
      "2024-03-28 14:34:47,796 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00026643120955702\n",
      "2024-03-28 14:34:47,796 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00026643120955702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3193.1717114448547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 4660/4660 [01:31<00:00, 51.17batch/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_tse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_nasdaq\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/tse_nasdaq_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2202bca5-a951-4694-b23a-1c124a9d998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 14:38:00,931 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 14:38:00,931 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 14:38:00,931 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 14:38:00,931 - INFO - Start Training on Domain 0...\n",
      "100% 1691/1691 [04:49<00:00,  5.84batch/s]\n",
      "2024-03-28 14:42:50,519 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.18575929325307478\n",
      "2024-03-28 14:42:50,519 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.18575929325307478\n",
      "2024-03-28 14:42:50,519 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.18575929325307478\n",
      "2024-03-28 14:42:50,519 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.18575929325307478\n",
      "100% 1691/1691 [04:57<00:00,  5.68batch/s]\n",
      "2024-03-28 14:47:48,220 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10020671693563003\n",
      "2024-03-28 14:47:48,220 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10020671693563003\n",
      "2024-03-28 14:47:48,220 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10020671693563003\n",
      "2024-03-28 14:47:48,220 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10020671693563003\n",
      "100% 1691/1691 [04:57<00:00,  5.69batch/s]\n",
      "2024-03-28 14:52:45,228 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07075630564175776\n",
      "2024-03-28 14:52:45,228 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07075630564175776\n",
      "2024-03-28 14:52:45,228 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07075630564175776\n",
      "2024-03-28 14:52:45,228 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07075630564175776\n",
      "2024-03-28 14:52:45,318 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 14:52:45,318 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 14:52:45,318 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 14:52:45,318 - INFO - Start Training on Domain 1...\n",
      "100% 1990/1990 [05:51<00:00,  5.67batch/s]\n",
      "2024-03-28 14:58:36,471 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.006922413522012569\n",
      "2024-03-28 14:58:36,471 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.006922413522012569\n",
      "2024-03-28 14:58:36,471 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.006922413522012569\n",
      "2024-03-28 14:58:36,471 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.006922413522012569\n",
      "100% 1990/1990 [05:49<00:00,  5.69batch/s]\n",
      "2024-03-28 15:04:26,004 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0036337770928801904\n",
      "2024-03-28 15:04:26,004 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0036337770928801904\n",
      "2024-03-28 15:04:26,004 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0036337770928801904\n",
      "2024-03-28 15:04:26,004 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0036337770928801904\n",
      "100% 1990/1990 [05:51<00:00,  5.67batch/s]\n",
      "2024-03-28 15:10:17,288 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.006040175382353652\n",
      "2024-03-28 15:10:17,288 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.006040175382353652\n",
      "2024-03-28 15:10:17,288 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.006040175382353652\n",
      "2024-03-28 15:10:17,288 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.006040175382353652\n",
      "2024-03-28 15:10:17,380 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 15:10:17,380 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 15:10:17,380 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 15:10:17,380 - INFO - Start Training on Domain 2...\n",
      "100% 2086/2086 [06:04<00:00,  5.73batch/s]\n",
      "2024-03-28 15:16:21,587 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00024752718265224624\n",
      "2024-03-28 15:16:21,587 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00024752718265224624\n",
      "2024-03-28 15:16:21,587 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00024752718265224624\n",
      "2024-03-28 15:16:21,587 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00024752718265224624\n",
      "100% 2086/2086 [06:04<00:00,  5.73batch/s]\n",
      "2024-03-28 15:22:25,633 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0003697567423832382\n",
      "2024-03-28 15:22:25,633 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0003697567423832382\n",
      "2024-03-28 15:22:25,633 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0003697567423832382\n",
      "2024-03-28 15:22:25,633 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0003697567423832382\n",
      "100% 2086/2086 [06:02<00:00,  5.75batch/s]\n",
      "2024-03-28 15:28:28,343 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00025355859343218477\n",
      "2024-03-28 15:28:28,343 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00025355859343218477\n",
      "2024-03-28 15:28:28,343 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00025355859343218477\n",
      "2024-03-28 15:28:28,343 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00025355859343218477\n",
      "2024-03-28 15:28:28,431 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 15:28:28,431 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 15:28:28,431 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 15:28:28,431 - INFO - Start Training on Domain 3...\n",
      "100% 2094/2094 [06:05<00:00,  5.73batch/s]\n",
      "2024-03-28 15:34:33,725 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 5.3654614312942526e-06\n",
      "2024-03-28 15:34:33,725 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 5.3654614312942526e-06\n",
      "2024-03-28 15:34:33,725 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 5.3654614312942526e-06\n",
      "2024-03-28 15:34:33,725 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 5.3654614312942526e-06\n",
      "100% 2094/2094 [06:05<00:00,  5.73batch/s]\n",
      "2024-03-28 15:40:39,472 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.565586133795117e-05\n",
      "2024-03-28 15:40:39,472 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.565586133795117e-05\n",
      "2024-03-28 15:40:39,472 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.565586133795117e-05\n",
      "2024-03-28 15:40:39,472 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.565586133795117e-05\n",
      "100% 2094/2094 [06:06<00:00,  5.72batch/s]\n",
      "2024-03-28 15:46:45,524 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.4115786831665857e-05\n",
      "2024-03-28 15:46:45,524 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.4115786831665857e-05\n",
      "2024-03-28 15:46:45,524 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.4115786831665857e-05\n",
      "2024-03-28 15:46:45,524 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.4115786831665857e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 4124.696045160294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 4660/4660 [01:25<00:00, 54.76batch/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_nasdaq\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_nasdaq\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/nasdaq_nasdaq_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad743e8-cd60-413e-82d5-2140034e26d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 15:49:32,419 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 15:49:32,419 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 15:49:32,419 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 15:49:32,419 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 15:49:32,419 - INFO - Start Training on Domain 0...\n",
      "100% 1691/1691 [04:57<00:00,  5.69batch/s]\n",
      "2024-03-28 15:54:29,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.17000553802774898\n",
      "2024-03-28 15:54:29,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.17000553802774898\n",
      "2024-03-28 15:54:29,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.17000553802774898\n",
      "2024-03-28 15:54:29,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.17000553802774898\n",
      "2024-03-28 15:54:29,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.17000553802774898\n",
      "100% 1691/1691 [05:03<00:00,  5.57batch/s]\n",
      "2024-03-28 15:59:33,246 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.09816078691551101\n",
      "2024-03-28 15:59:33,246 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.09816078691551101\n",
      "2024-03-28 15:59:33,246 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.09816078691551101\n",
      "2024-03-28 15:59:33,246 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.09816078691551101\n",
      "2024-03-28 15:59:33,246 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.09816078691551101\n",
      "100% 1691/1691 [05:02<00:00,  5.58batch/s]\n",
      "2024-03-28 16:04:36,080 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.06551998757760735\n",
      "2024-03-28 16:04:36,080 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.06551998757760735\n",
      "2024-03-28 16:04:36,080 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.06551998757760735\n",
      "2024-03-28 16:04:36,080 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.06551998757760735\n",
      "2024-03-28 16:04:36,080 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.06551998757760735\n",
      "2024-03-28 16:04:36,173 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 16:04:36,173 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 16:04:36,173 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 16:04:36,173 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 16:04:36,173 - INFO - Start Training on Domain 1...\n",
      "100% 1990/1990 [05:57<00:00,  5.57batch/s]\n",
      "2024-03-28 16:10:33,480 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.009867911161059261\n",
      "2024-03-28 16:10:33,480 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.009867911161059261\n",
      "2024-03-28 16:10:33,480 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.009867911161059261\n",
      "2024-03-28 16:10:33,480 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.009867911161059261\n",
      "2024-03-28 16:10:33,480 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.009867911161059261\n",
      "100% 1990/1990 [05:56<00:00,  5.58batch/s]\n",
      "2024-03-28 16:16:30,135 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.010426736761109164\n",
      "2024-03-28 16:16:30,135 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.010426736761109164\n",
      "2024-03-28 16:16:30,135 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.010426736761109164\n",
      "2024-03-28 16:16:30,135 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.010426736761109164\n",
      "2024-03-28 16:16:30,135 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.010426736761109164\n",
      "100% 1990/1990 [05:56<00:00,  5.59batch/s]\n",
      "2024-03-28 16:22:26,304 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009257436238098235\n",
      "2024-03-28 16:22:26,304 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009257436238098235\n",
      "2024-03-28 16:22:26,304 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009257436238098235\n",
      "2024-03-28 16:22:26,304 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009257436238098235\n",
      "2024-03-28 16:22:26,304 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009257436238098235\n",
      "2024-03-28 16:22:26,395 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 16:22:26,395 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 16:22:26,395 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 16:22:26,395 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 16:22:26,395 - INFO - Start Training on Domain 2...\n",
      "100% 2086/2086 [06:13<00:00,  5.58batch/s]\n",
      "2024-03-28 16:28:40,050 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00033021691252592655\n",
      "2024-03-28 16:28:40,050 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00033021691252592655\n",
      "2024-03-28 16:28:40,050 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00033021691252592655\n",
      "2024-03-28 16:28:40,050 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00033021691252592655\n",
      "2024-03-28 16:28:40,050 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00033021691252592655\n",
      "100% 2086/2086 [06:12<00:00,  5.60batch/s]\n",
      "2024-03-28 16:34:52,397 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0001950194882380768\n",
      "2024-03-28 16:34:52,397 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0001950194882380768\n",
      "2024-03-28 16:34:52,397 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0001950194882380768\n",
      "2024-03-28 16:34:52,397 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0001950194882380768\n",
      "2024-03-28 16:34:52,397 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0001950194882380768\n",
      "100% 2086/2086 [06:12<00:00,  5.61batch/s]\n",
      "2024-03-28 16:41:04,569 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00015165590490346978\n",
      "2024-03-28 16:41:04,569 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00015165590490346978\n",
      "2024-03-28 16:41:04,569 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00015165590490346978\n",
      "2024-03-28 16:41:04,569 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00015165590490346978\n",
      "2024-03-28 16:41:04,569 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00015165590490346978\n",
      "2024-03-28 16:41:04,677 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 16:41:04,677 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 16:41:04,677 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 16:41:04,677 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 16:41:04,677 - INFO - Start Training on Domain 3...\n",
      "100% 2094/2094 [06:12<00:00,  5.62batch/s]\n",
      "2024-03-28 16:47:17,152 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 8.250018288207942e-06\n",
      "2024-03-28 16:47:17,152 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 8.250018288207942e-06\n",
      "2024-03-28 16:47:17,152 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 8.250018288207942e-06\n",
      "2024-03-28 16:47:17,152 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 8.250018288207942e-06\n",
      "2024-03-28 16:47:17,152 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 8.250018288207942e-06\n",
      "100% 2094/2094 [06:10<00:00,  5.65batch/s]\n",
      "2024-03-28 16:53:27,944 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 2.7912989554085273e-05\n",
      "2024-03-28 16:53:27,944 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 2.7912989554085273e-05\n",
      "2024-03-28 16:53:27,944 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 2.7912989554085273e-05\n",
      "2024-03-28 16:53:27,944 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 2.7912989554085273e-05\n",
      "2024-03-28 16:53:27,944 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 2.7912989554085273e-05\n",
      "100% 2094/2094 [06:14<00:00,  5.59batch/s]\n",
      "2024-03-28 16:59:42,319 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.1513340658478886e-05\n",
      "2024-03-28 16:59:42,319 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.1513340658478886e-05\n",
      "2024-03-28 16:59:42,319 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.1513340658478886e-05\n",
      "2024-03-28 16:59:42,319 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.1513340658478886e-05\n",
      "2024-03-28 16:59:42,319 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.1513340658478886e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 4210.011409282684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1221/1221 [00:22<00:00, 53.59batch/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_nasdaq\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_sse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/nasdaq_sse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11aa786c-1a40-4ddc-bfb4-9ff689910cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 17:01:11,973 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 17:01:11,973 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 17:01:11,973 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 17:01:11,973 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 17:01:11,973 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 17:01:11,973 - INFO - Start Training on Domain 0...\n",
      "100% 1691/1691 [04:49<00:00,  5.84batch/s]\n",
      "2024-03-28 17:06:01,379 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.20332864772347764\n",
      "2024-03-28 17:06:01,379 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.20332864772347764\n",
      "2024-03-28 17:06:01,379 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.20332864772347764\n",
      "2024-03-28 17:06:01,379 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.20332864772347764\n",
      "2024-03-28 17:06:01,379 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.20332864772347764\n",
      "2024-03-28 17:06:01,379 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.20332864772347764\n",
      "100% 1691/1691 [04:58<00:00,  5.67batch/s]\n",
      "2024-03-28 17:10:59,442 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10614203514078537\n",
      "2024-03-28 17:10:59,442 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10614203514078537\n",
      "2024-03-28 17:10:59,442 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10614203514078537\n",
      "2024-03-28 17:10:59,442 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10614203514078537\n",
      "2024-03-28 17:10:59,442 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10614203514078537\n",
      "2024-03-28 17:10:59,442 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.10614203514078537\n",
      "100% 1691/1691 [04:56<00:00,  5.70batch/s]\n",
      "2024-03-28 17:15:56,277 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07230102201042891\n",
      "2024-03-28 17:15:56,277 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07230102201042891\n",
      "2024-03-28 17:15:56,277 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07230102201042891\n",
      "2024-03-28 17:15:56,277 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07230102201042891\n",
      "2024-03-28 17:15:56,277 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07230102201042891\n",
      "2024-03-28 17:15:56,277 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.07230102201042891\n",
      "2024-03-28 17:15:56,368 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 17:15:56,368 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 17:15:56,368 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 17:15:56,368 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 17:15:56,368 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 17:15:56,368 - INFO - Start Training on Domain 1...\n",
      "100% 1990/1990 [05:51<00:00,  5.66batch/s]\n",
      "2024-03-28 17:21:47,923 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.014636856740739562\n",
      "2024-03-28 17:21:47,923 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.014636856740739562\n",
      "2024-03-28 17:21:47,923 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.014636856740739562\n",
      "2024-03-28 17:21:47,923 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.014636856740739562\n",
      "2024-03-28 17:21:47,923 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.014636856740739562\n",
      "2024-03-28 17:21:47,923 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.014636856740739562\n",
      "100% 1990/1990 [05:49<00:00,  5.70batch/s]\n",
      "2024-03-28 17:27:37,282 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.011551672118863947\n",
      "2024-03-28 17:27:37,282 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.011551672118863947\n",
      "2024-03-28 17:27:37,282 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.011551672118863947\n",
      "2024-03-28 17:27:37,282 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.011551672118863947\n",
      "2024-03-28 17:27:37,282 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.011551672118863947\n",
      "2024-03-28 17:27:37,282 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.011551672118863947\n",
      "100% 1990/1990 [05:48<00:00,  5.70batch/s]\n",
      "2024-03-28 17:33:26,168 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009470211763236784\n",
      "2024-03-28 17:33:26,168 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009470211763236784\n",
      "2024-03-28 17:33:26,168 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009470211763236784\n",
      "2024-03-28 17:33:26,168 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009470211763236784\n",
      "2024-03-28 17:33:26,168 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009470211763236784\n",
      "2024-03-28 17:33:26,168 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.009470211763236784\n",
      "2024-03-28 17:33:26,274 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 17:33:26,274 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 17:33:26,274 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 17:33:26,274 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 17:33:26,274 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 17:33:26,274 - INFO - Start Training on Domain 2...\n",
      "100% 2086/2086 [06:05<00:00,  5.70batch/s]\n",
      "2024-03-28 17:39:32,118 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0013684084485369812\n",
      "2024-03-28 17:39:32,118 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0013684084485369812\n",
      "2024-03-28 17:39:32,118 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0013684084485369812\n",
      "2024-03-28 17:39:32,118 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0013684084485369812\n",
      "2024-03-28 17:39:32,118 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0013684084485369812\n",
      "2024-03-28 17:39:32,118 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0013684084485369812\n",
      "100% 2086/2086 [06:05<00:00,  5.70batch/s]\n",
      "2024-03-28 17:45:37,895 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.001012513068462664\n",
      "2024-03-28 17:45:37,895 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.001012513068462664\n",
      "2024-03-28 17:45:37,895 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.001012513068462664\n",
      "2024-03-28 17:45:37,895 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.001012513068462664\n",
      "2024-03-28 17:45:37,895 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.001012513068462664\n",
      "2024-03-28 17:45:37,895 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.001012513068462664\n",
      "100% 2086/2086 [06:06<00:00,  5.69batch/s]\n",
      "2024-03-28 17:51:44,738 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008941535903727258\n",
      "2024-03-28 17:51:44,738 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008941535903727258\n",
      "2024-03-28 17:51:44,738 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008941535903727258\n",
      "2024-03-28 17:51:44,738 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008941535903727258\n",
      "2024-03-28 17:51:44,738 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008941535903727258\n",
      "2024-03-28 17:51:44,738 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008941535903727258\n",
      "2024-03-28 17:51:44,834 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 17:51:44,834 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 17:51:44,834 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 17:51:44,834 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 17:51:44,834 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 17:51:44,834 - INFO - Start Training on Domain 3...\n",
      "100% 2094/2094 [06:10<00:00,  5.65batch/s]\n",
      "2024-03-28 17:57:55,695 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 2.1021969708922712e-05\n",
      "2024-03-28 17:57:55,695 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 2.1021969708922712e-05\n",
      "2024-03-28 17:57:55,695 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 2.1021969708922712e-05\n",
      "2024-03-28 17:57:55,695 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 2.1021969708922712e-05\n",
      "2024-03-28 17:57:55,695 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 2.1021969708922712e-05\n",
      "2024-03-28 17:57:55,695 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 2.1021969708922712e-05\n",
      "100% 2094/2094 [06:10<00:00,  5.65batch/s]\n",
      "2024-03-28 18:04:06,414 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 1.7566861693303473e-05\n",
      "2024-03-28 18:04:06,414 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 1.7566861693303473e-05\n",
      "2024-03-28 18:04:06,414 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 1.7566861693303473e-05\n",
      "2024-03-28 18:04:06,414 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 1.7566861693303473e-05\n",
      "2024-03-28 18:04:06,414 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 1.7566861693303473e-05\n",
      "2024-03-28 18:04:06,414 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 1.7566861693303473e-05\n",
      "100% 2094/2094 [06:11<00:00,  5.64batch/s]\n",
      "2024-03-28 18:10:17,775 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 1.708263414504231e-05\n",
      "2024-03-28 18:10:17,775 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 1.708263414504231e-05\n",
      "2024-03-28 18:10:17,775 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 1.708263414504231e-05\n",
      "2024-03-28 18:10:17,775 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 1.708263414504231e-05\n",
      "2024-03-28 18:10:17,775 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 1.708263414504231e-05\n",
      "2024-03-28 18:10:17,775 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 1.708263414504231e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 4145.893763303757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1599/1599 [00:29<00:00, 54.49batch/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_nasdaq\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_szse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/nasdaq_szse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "107e21d1-775b-42b9-afd5-998522c54257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 18:12:06,318 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 18:12:06,318 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 18:12:06,318 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 18:12:06,318 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 18:12:06,318 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 18:12:06,318 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 18:12:06,318 - INFO - Start Training on Domain 0...\n",
      "100% 1691/1691 [05:01<00:00,  5.60batch/s]\n",
      "2024-03-28 18:17:08,178 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.12011917662822781\n",
      "2024-03-28 18:17:08,178 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.12011917662822781\n",
      "2024-03-28 18:17:08,178 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.12011917662822781\n",
      "2024-03-28 18:17:08,178 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.12011917662822781\n",
      "2024-03-28 18:17:08,178 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.12011917662822781\n",
      "2024-03-28 18:17:08,178 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.12011917662822781\n",
      "2024-03-28 18:17:08,178 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.12011917662822781\n",
      "100% 1691/1691 [05:19<00:00,  5.30batch/s]\n",
      "2024-03-28 18:22:27,418 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.0799630865612189\n",
      "2024-03-28 18:22:27,418 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.0799630865612189\n",
      "2024-03-28 18:22:27,418 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.0799630865612189\n",
      "2024-03-28 18:22:27,418 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.0799630865612189\n",
      "2024-03-28 18:22:27,418 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.0799630865612189\n",
      "2024-03-28 18:22:27,418 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.0799630865612189\n",
      "2024-03-28 18:22:27,418 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.0799630865612189\n",
      "100% 1691/1691 [06:03<00:00,  4.65batch/s]\n",
      "2024-03-28 18:28:31,346 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.054003383831018034\n",
      "2024-03-28 18:28:31,346 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.054003383831018034\n",
      "2024-03-28 18:28:31,346 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.054003383831018034\n",
      "2024-03-28 18:28:31,346 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.054003383831018034\n",
      "2024-03-28 18:28:31,346 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.054003383831018034\n",
      "2024-03-28 18:28:31,346 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.054003383831018034\n",
      "2024-03-28 18:28:31,346 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.054003383831018034\n",
      "2024-03-28 18:28:31,595 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 18:28:31,595 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 18:28:31,595 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 18:28:31,595 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 18:28:31,595 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 18:28:31,595 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 18:28:31,595 - INFO - Start Training on Domain 1...\n",
      "100% 1990/1990 [06:04<00:00,  5.46batch/s]\n",
      "2024-03-28 18:34:36,020 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00044851828385238075\n",
      "2024-03-28 18:34:36,020 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00044851828385238075\n",
      "2024-03-28 18:34:36,020 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00044851828385238075\n",
      "2024-03-28 18:34:36,020 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00044851828385238075\n",
      "2024-03-28 18:34:36,020 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00044851828385238075\n",
      "2024-03-28 18:34:36,020 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00044851828385238075\n",
      "2024-03-28 18:34:36,020 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00044851828385238075\n",
      "100% 1990/1990 [09:13<00:00,  3.59batch/s]\n",
      "2024-03-28 18:43:49,801 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0005945520710677433\n",
      "2024-03-28 18:43:49,801 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0005945520710677433\n",
      "2024-03-28 18:43:49,801 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0005945520710677433\n",
      "2024-03-28 18:43:49,801 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0005945520710677433\n",
      "2024-03-28 18:43:49,801 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0005945520710677433\n",
      "2024-03-28 18:43:49,801 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0005945520710677433\n",
      "2024-03-28 18:43:49,801 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0005945520710677433\n",
      "100% 1990/1990 [09:22<00:00,  3.54batch/s]\n",
      "2024-03-28 18:53:12,007 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002244844137483481\n",
      "2024-03-28 18:53:12,007 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002244844137483481\n",
      "2024-03-28 18:53:12,007 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002244844137483481\n",
      "2024-03-28 18:53:12,007 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002244844137483481\n",
      "2024-03-28 18:53:12,007 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002244844137483481\n",
      "2024-03-28 18:53:12,007 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002244844137483481\n",
      "2024-03-28 18:53:12,007 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002244844137483481\n",
      "2024-03-28 18:53:12,179 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 18:53:12,179 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 18:53:12,179 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 18:53:12,179 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 18:53:12,179 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 18:53:12,179 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 18:53:12,179 - INFO - Start Training on Domain 2...\n",
      "100% 2086/2086 [09:49<00:00,  3.54batch/s]\n",
      "2024-03-28 19:03:01,457 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00023631970304370726\n",
      "2024-03-28 19:03:01,457 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00023631970304370726\n",
      "2024-03-28 19:03:01,457 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00023631970304370726\n",
      "2024-03-28 19:03:01,457 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00023631970304370726\n",
      "2024-03-28 19:03:01,457 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00023631970304370726\n",
      "2024-03-28 19:03:01,457 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00023631970304370726\n",
      "2024-03-28 19:03:01,457 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.00023631970304370726\n",
      "100% 2086/2086 [06:09<00:00,  5.65batch/s]\n",
      "2024-03-28 19:09:10,925 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.00013751229664180444\n",
      "2024-03-28 19:09:10,925 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.00013751229664180444\n",
      "2024-03-28 19:09:10,925 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.00013751229664180444\n",
      "2024-03-28 19:09:10,925 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.00013751229664180444\n",
      "2024-03-28 19:09:10,925 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.00013751229664180444\n",
      "2024-03-28 19:09:10,925 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.00013751229664180444\n",
      "2024-03-28 19:09:10,925 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.00013751229664180444\n",
      "100% 2086/2086 [06:05<00:00,  5.71batch/s]\n",
      "2024-03-28 19:15:16,287 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00010096948009285542\n",
      "2024-03-28 19:15:16,287 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00010096948009285542\n",
      "2024-03-28 19:15:16,287 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00010096948009285542\n",
      "2024-03-28 19:15:16,287 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00010096948009285542\n",
      "2024-03-28 19:15:16,287 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00010096948009285542\n",
      "2024-03-28 19:15:16,287 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00010096948009285542\n",
      "2024-03-28 19:15:16,287 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00010096948009285542\n",
      "2024-03-28 19:15:16,424 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:15:16,424 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:15:16,424 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:15:16,424 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:15:16,424 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:15:16,424 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:15:16,424 - INFO - Start Training on Domain 3...\n",
      "100% 2094/2094 [06:11<00:00,  5.64batch/s]\n",
      "2024-03-28 19:21:27,653 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 3.015568236168385e-06\n",
      "2024-03-28 19:21:27,653 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 3.015568236168385e-06\n",
      "2024-03-28 19:21:27,653 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 3.015568236168385e-06\n",
      "2024-03-28 19:21:27,653 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 3.015568236168385e-06\n",
      "2024-03-28 19:21:27,653 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 3.015568236168385e-06\n",
      "2024-03-28 19:21:27,653 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 3.015568236168385e-06\n",
      "2024-03-28 19:21:27,653 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 3.015568236168385e-06\n",
      "100% 2094/2094 [06:07<00:00,  5.71batch/s]\n",
      "2024-03-28 19:27:34,680 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.715773596078755e-06\n",
      "2024-03-28 19:27:34,680 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.715773596078755e-06\n",
      "2024-03-28 19:27:34,680 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.715773596078755e-06\n",
      "2024-03-28 19:27:34,680 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.715773596078755e-06\n",
      "2024-03-28 19:27:34,680 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.715773596078755e-06\n",
      "2024-03-28 19:27:34,680 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.715773596078755e-06\n",
      "2024-03-28 19:27:34,680 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 3.715773596078755e-06\n",
      "100% 2094/2094 [06:06<00:00,  5.71batch/s]\n",
      "2024-03-28 19:33:41,390 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.5210728639347163e-06\n",
      "2024-03-28 19:33:41,390 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.5210728639347163e-06\n",
      "2024-03-28 19:33:41,390 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.5210728639347163e-06\n",
      "2024-03-28 19:33:41,390 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.5210728639347163e-06\n",
      "2024-03-28 19:33:41,390 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.5210728639347163e-06\n",
      "2024-03-28 19:33:41,390 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.5210728639347163e-06\n",
      "2024-03-28 19:33:41,390 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 2.5210728639347163e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 4895.21611905098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 3592/3592 [02:16<00:00, 26.38batch/s]\n"
     ]
    }
   ],
   "source": [
    "test# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_nasdaq\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_tse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/nasdaq_tse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbd2f915-cb83-4702-bf19-4df2e4441300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 19:36:42,202 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 19:36:42,202 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 19:36:42,202 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 19:36:42,202 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 19:36:42,202 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 19:36:42,202 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 19:36:42,202 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 19:36:42,202 - INFO - Start Training on Domain 0...\n",
      "100% 430/430 [01:10<00:00,  6.12batch/s]\n",
      "2024-03-28 19:37:52,423 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6148884445791631\n",
      "2024-03-28 19:37:52,423 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6148884445791631\n",
      "2024-03-28 19:37:52,423 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6148884445791631\n",
      "2024-03-28 19:37:52,423 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6148884445791631\n",
      "2024-03-28 19:37:52,423 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6148884445791631\n",
      "2024-03-28 19:37:52,423 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6148884445791631\n",
      "2024-03-28 19:37:52,423 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6148884445791631\n",
      "2024-03-28 19:37:52,423 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6148884445791631\n",
      "100% 430/430 [01:12<00:00,  5.96batch/s]\n",
      "2024-03-28 19:39:04,582 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3471280197090426\n",
      "2024-03-28 19:39:04,582 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3471280197090426\n",
      "2024-03-28 19:39:04,582 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3471280197090426\n",
      "2024-03-28 19:39:04,582 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3471280197090426\n",
      "2024-03-28 19:39:04,582 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3471280197090426\n",
      "2024-03-28 19:39:04,582 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3471280197090426\n",
      "2024-03-28 19:39:04,582 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3471280197090426\n",
      "2024-03-28 19:39:04,582 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3471280197090426\n",
      "100% 430/430 [01:13<00:00,  5.82batch/s]\n",
      "2024-03-28 19:40:18,411 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.23366093016608597\n",
      "2024-03-28 19:40:18,411 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.23366093016608597\n",
      "2024-03-28 19:40:18,411 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.23366093016608597\n",
      "2024-03-28 19:40:18,411 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.23366093016608597\n",
      "2024-03-28 19:40:18,411 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.23366093016608597\n",
      "2024-03-28 19:40:18,411 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.23366093016608597\n",
      "2024-03-28 19:40:18,411 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.23366093016608597\n",
      "2024-03-28 19:40:18,411 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.23366093016608597\n",
      "2024-03-28 19:40:18,574 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 19:40:18,574 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 19:40:18,574 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 19:40:18,574 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 19:40:18,574 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 19:40:18,574 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 19:40:18,574 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 19:40:18,574 - INFO - Start Training on Domain 1...\n",
      "100% 513/513 [01:30<00:00,  5.68batch/s]\n",
      "2024-03-28 19:41:48,830 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00038563034208545906\n",
      "2024-03-28 19:41:48,830 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00038563034208545906\n",
      "2024-03-28 19:41:48,830 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00038563034208545906\n",
      "2024-03-28 19:41:48,830 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00038563034208545906\n",
      "2024-03-28 19:41:48,830 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00038563034208545906\n",
      "2024-03-28 19:41:48,830 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00038563034208545906\n",
      "2024-03-28 19:41:48,830 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00038563034208545906\n",
      "2024-03-28 19:41:48,830 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.00038563034208545906\n",
      "100% 513/513 [01:29<00:00,  5.73batch/s]\n",
      "2024-03-28 19:43:18,375 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.001693439631782777\n",
      "2024-03-28 19:43:18,375 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.001693439631782777\n",
      "2024-03-28 19:43:18,375 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.001693439631782777\n",
      "2024-03-28 19:43:18,375 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.001693439631782777\n",
      "2024-03-28 19:43:18,375 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.001693439631782777\n",
      "2024-03-28 19:43:18,375 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.001693439631782777\n",
      "2024-03-28 19:43:18,375 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.001693439631782777\n",
      "2024-03-28 19:43:18,375 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.001693439631782777\n",
      "100% 513/513 [01:28<00:00,  5.78batch/s]\n",
      "2024-03-28 19:44:47,109 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.021783631675603755\n",
      "2024-03-28 19:44:47,109 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.021783631675603755\n",
      "2024-03-28 19:44:47,109 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.021783631675603755\n",
      "2024-03-28 19:44:47,109 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.021783631675603755\n",
      "2024-03-28 19:44:47,109 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.021783631675603755\n",
      "2024-03-28 19:44:47,109 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.021783631675603755\n",
      "2024-03-28 19:44:47,109 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.021783631675603755\n",
      "2024-03-28 19:44:47,109 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.021783631675603755\n",
      "2024-03-28 19:44:47,290 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 19:44:47,290 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 19:44:47,290 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 19:44:47,290 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 19:44:47,290 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 19:44:47,290 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 19:44:47,290 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 19:44:47,290 - INFO - Start Training on Domain 2...\n",
      "100% 548/548 [01:33<00:00,  5.88batch/s]\n",
      "2024-03-28 19:46:20,463 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0011453361957449548\n",
      "2024-03-28 19:46:20,463 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0011453361957449548\n",
      "2024-03-28 19:46:20,463 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0011453361957449548\n",
      "2024-03-28 19:46:20,463 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0011453361957449548\n",
      "2024-03-28 19:46:20,463 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0011453361957449548\n",
      "2024-03-28 19:46:20,463 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0011453361957449548\n",
      "2024-03-28 19:46:20,463 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0011453361957449548\n",
      "2024-03-28 19:46:20,463 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0011453361957449548\n",
      "100% 548/548 [01:36<00:00,  5.66batch/s]\n",
      "2024-03-28 19:47:57,361 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0008213374547891482\n",
      "2024-03-28 19:47:57,361 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0008213374547891482\n",
      "2024-03-28 19:47:57,361 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0008213374547891482\n",
      "2024-03-28 19:47:57,361 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0008213374547891482\n",
      "2024-03-28 19:47:57,361 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0008213374547891482\n",
      "2024-03-28 19:47:57,361 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0008213374547891482\n",
      "2024-03-28 19:47:57,361 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0008213374547891482\n",
      "2024-03-28 19:47:57,361 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0008213374547891482\n",
      "100% 548/548 [01:37<00:00,  5.65batch/s]\n",
      "2024-03-28 19:49:34,389 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0015797461841908032\n",
      "2024-03-28 19:49:34,389 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0015797461841908032\n",
      "2024-03-28 19:49:34,389 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0015797461841908032\n",
      "2024-03-28 19:49:34,389 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0015797461841908032\n",
      "2024-03-28 19:49:34,389 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0015797461841908032\n",
      "2024-03-28 19:49:34,389 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0015797461841908032\n",
      "2024-03-28 19:49:34,389 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0015797461841908032\n",
      "2024-03-28 19:49:34,389 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0015797461841908032\n",
      "2024-03-28 19:49:34,551 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:49:34,551 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:49:34,551 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:49:34,551 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:49:34,551 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:49:34,551 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:49:34,551 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 19:49:34,551 - INFO - Start Training on Domain 3...\n",
      "100% 549/549 [01:36<00:00,  5.69batch/s]\n",
      "2024-03-28 19:51:11,054 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.001122631278492931\n",
      "2024-03-28 19:51:11,054 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.001122631278492931\n",
      "2024-03-28 19:51:11,054 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.001122631278492931\n",
      "2024-03-28 19:51:11,054 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.001122631278492931\n",
      "2024-03-28 19:51:11,054 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.001122631278492931\n",
      "2024-03-28 19:51:11,054 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.001122631278492931\n",
      "2024-03-28 19:51:11,054 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.001122631278492931\n",
      "2024-03-28 19:51:11,054 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.001122631278492931\n",
      "100% 549/549 [01:35<00:00,  5.75batch/s]\n",
      "2024-03-28 19:52:46,512 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0006847906508270943\n",
      "2024-03-28 19:52:46,512 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0006847906508270943\n",
      "2024-03-28 19:52:46,512 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0006847906508270943\n",
      "2024-03-28 19:52:46,512 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0006847906508270943\n",
      "2024-03-28 19:52:46,512 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0006847906508270943\n",
      "2024-03-28 19:52:46,512 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0006847906508270943\n",
      "2024-03-28 19:52:46,512 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0006847906508270943\n",
      "2024-03-28 19:52:46,512 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0006847906508270943\n",
      "100% 549/549 [01:35<00:00,  5.73batch/s]\n",
      "2024-03-28 19:54:22,411 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0005243290080018816\n",
      "2024-03-28 19:54:22,411 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0005243290080018816\n",
      "2024-03-28 19:54:22,411 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0005243290080018816\n",
      "2024-03-28 19:54:22,411 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0005243290080018816\n",
      "2024-03-28 19:54:22,411 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0005243290080018816\n",
      "2024-03-28 19:54:22,411 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0005243290080018816\n",
      "2024-03-28 19:54:22,411 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0005243290080018816\n",
      "2024-03-28 19:54:22,411 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0005243290080018816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1060.3688349723816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1221/1221 [00:24<00:00, 50.21batch/s]\n"
     ]
    }
   ],
   "source": [
    "test# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_sse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_sse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/sse_sse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22dc6b71-0149-4c2c-b497-757cd7d19f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 18:49:09,811 - INFO - Start Training on Domain 0...\n",
      "100% 430/430 [00:07<00:00, 58.28batch/s]\n",
      "2024-04-01 18:49:17,194 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.7454444737455179\n",
      "100% 430/430 [00:07<00:00, 60.57batch/s]\n",
      "2024-04-01 18:49:24,297 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.4420095902510247\n",
      "100% 430/430 [00:06<00:00, 62.17batch/s]\n",
      "2024-04-01 18:49:31,216 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.31093828257231054\n",
      "2024-04-01 18:49:31,291 - INFO - Start Training on Domain 1...\n",
      "100% 513/513 [00:08<00:00, 60.45batch/s]\n",
      "2024-04-01 18:49:39,781 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.004480823894443452\n",
      "100% 513/513 [00:08<00:00, 60.28batch/s]\n",
      "2024-04-01 18:49:48,296 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.008673737996546497\n",
      "100% 513/513 [00:08<00:00, 60.24batch/s]\n",
      "2024-04-01 18:49:56,816 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.005971750882929258\n",
      "2024-04-01 18:49:56,877 - INFO - Start Training on Domain 2...\n",
      "100% 548/548 [00:09<00:00, 60.41batch/s]\n",
      "2024-04-01 18:50:05,951 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0005044253070560032\n",
      "100% 548/548 [00:09<00:00, 59.98batch/s]\n",
      "2024-04-01 18:50:15,090 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0004546815057006413\n",
      "100% 548/548 [00:09<00:00, 60.01batch/s]\n",
      "2024-04-01 18:50:24,225 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00036522325348118073\n",
      "2024-04-01 18:50:24,286 - INFO - Start Training on Domain 3...\n",
      "100% 549/549 [00:09<00:00, 60.34batch/s]\n",
      "2024-04-01 18:50:33,388 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0012711070542425623\n",
      "100% 549/549 [00:09<00:00, 60.42batch/s]\n",
      "2024-04-01 18:50:42,478 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.0007064633787302589\n",
      "100% 549/549 [00:09<00:00, 60.24batch/s]\n",
      "2024-04-01 18:50:51,594 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0005511588704815835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 101.84341597557068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1599/1599 [00:07<00:00, 219.32batch/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cuda:0'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_sse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_szse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=64, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/sse_szse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aaaa231-faa2-4e53-ba66-96de7133d727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 20:14:29,001 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:14:29,001 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:14:29,001 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:14:29,001 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:14:29,001 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:14:29,001 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:14:29,001 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:14:29,001 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:14:29,001 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:14:29,001 - INFO - Start Training on Domain 0...\n",
      "100% 430/430 [01:11<00:00,  6.00batch/s]\n",
      "2024-03-28 20:15:40,664 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6386750003063055\n",
      "2024-03-28 20:15:40,664 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6386750003063055\n",
      "2024-03-28 20:15:40,664 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6386750003063055\n",
      "2024-03-28 20:15:40,664 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6386750003063055\n",
      "2024-03-28 20:15:40,664 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6386750003063055\n",
      "2024-03-28 20:15:40,664 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6386750003063055\n",
      "2024-03-28 20:15:40,664 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6386750003063055\n",
      "2024-03-28 20:15:40,664 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6386750003063055\n",
      "2024-03-28 20:15:40,664 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6386750003063055\n",
      "2024-03-28 20:15:40,664 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.6386750003063055\n",
      "100% 430/430 [01:14<00:00,  5.76batch/s]\n",
      "2024-03-28 20:16:55,345 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3364373183403121\n",
      "2024-03-28 20:16:55,345 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3364373183403121\n",
      "2024-03-28 20:16:55,345 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3364373183403121\n",
      "2024-03-28 20:16:55,345 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3364373183403121\n",
      "2024-03-28 20:16:55,345 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3364373183403121\n",
      "2024-03-28 20:16:55,345 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3364373183403121\n",
      "2024-03-28 20:16:55,345 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3364373183403121\n",
      "2024-03-28 20:16:55,345 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3364373183403121\n",
      "2024-03-28 20:16:55,345 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3364373183403121\n",
      "2024-03-28 20:16:55,345 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.3364373183403121\n",
      "100% 430/430 [01:17<00:00,  5.57batch/s]\n",
      "2024-03-28 20:18:12,536 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.22500004163810572\n",
      "2024-03-28 20:18:12,536 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.22500004163810572\n",
      "2024-03-28 20:18:12,536 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.22500004163810572\n",
      "2024-03-28 20:18:12,536 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.22500004163810572\n",
      "2024-03-28 20:18:12,536 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.22500004163810572\n",
      "2024-03-28 20:18:12,536 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.22500004163810572\n",
      "2024-03-28 20:18:12,536 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.22500004163810572\n",
      "2024-03-28 20:18:12,536 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.22500004163810572\n",
      "2024-03-28 20:18:12,536 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.22500004163810572\n",
      "2024-03-28 20:18:12,536 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.22500004163810572\n",
      "2024-03-28 20:18:12,631 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:18:12,631 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:18:12,631 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:18:12,631 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:18:12,631 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:18:12,631 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:18:12,631 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:18:12,631 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:18:12,631 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:18:12,631 - INFO - Start Training on Domain 1...\n",
      "100% 513/513 [01:29<00:00,  5.71batch/s]\n",
      "2024-03-28 20:19:42,453 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.003882994216305819\n",
      "2024-03-28 20:19:42,453 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.003882994216305819\n",
      "2024-03-28 20:19:42,453 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.003882994216305819\n",
      "2024-03-28 20:19:42,453 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.003882994216305819\n",
      "2024-03-28 20:19:42,453 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.003882994216305819\n",
      "2024-03-28 20:19:42,453 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.003882994216305819\n",
      "2024-03-28 20:19:42,453 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.003882994216305819\n",
      "2024-03-28 20:19:42,453 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.003882994216305819\n",
      "2024-03-28 20:19:42,453 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.003882994216305819\n",
      "2024-03-28 20:19:42,453 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.003882994216305819\n",
      "100% 513/513 [01:32<00:00,  5.56batch/s]\n",
      "2024-03-28 20:21:14,701 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.003637480963435438\n",
      "2024-03-28 20:21:14,701 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.003637480963435438\n",
      "2024-03-28 20:21:14,701 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.003637480963435438\n",
      "2024-03-28 20:21:14,701 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.003637480963435438\n",
      "2024-03-28 20:21:14,701 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.003637480963435438\n",
      "2024-03-28 20:21:14,701 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.003637480963435438\n",
      "2024-03-28 20:21:14,701 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.003637480963435438\n",
      "2024-03-28 20:21:14,701 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.003637480963435438\n",
      "2024-03-28 20:21:14,701 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.003637480963435438\n",
      "2024-03-28 20:21:14,701 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.003637480963435438\n",
      "100% 513/513 [01:30<00:00,  5.64batch/s]\n",
      "2024-03-28 20:22:45,640 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002981290539522075\n",
      "2024-03-28 20:22:45,640 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002981290539522075\n",
      "2024-03-28 20:22:45,640 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002981290539522075\n",
      "2024-03-28 20:22:45,640 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002981290539522075\n",
      "2024-03-28 20:22:45,640 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002981290539522075\n",
      "2024-03-28 20:22:45,640 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002981290539522075\n",
      "2024-03-28 20:22:45,640 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002981290539522075\n",
      "2024-03-28 20:22:45,640 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002981290539522075\n",
      "2024-03-28 20:22:45,640 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002981290539522075\n",
      "2024-03-28 20:22:45,640 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.002981290539522075\n",
      "2024-03-28 20:22:45,765 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:22:45,765 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:22:45,765 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:22:45,765 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:22:45,765 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:22:45,765 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:22:45,765 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:22:45,765 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:22:45,765 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:22:45,765 - INFO - Start Training on Domain 2...\n",
      "100% 548/548 [02:32<00:00,  3.60batch/s]\n",
      "2024-03-28 20:25:18,013 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.06255698467292109\n",
      "2024-03-28 20:25:18,013 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.06255698467292109\n",
      "2024-03-28 20:25:18,013 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.06255698467292109\n",
      "2024-03-28 20:25:18,013 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.06255698467292109\n",
      "2024-03-28 20:25:18,013 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.06255698467292109\n",
      "2024-03-28 20:25:18,013 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.06255698467292109\n",
      "2024-03-28 20:25:18,013 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.06255698467292109\n",
      "2024-03-28 20:25:18,013 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.06255698467292109\n",
      "2024-03-28 20:25:18,013 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.06255698467292109\n",
      "2024-03-28 20:25:18,013 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.06255698467292109\n",
      "100% 548/548 [02:37<00:00,  3.47batch/s]\n",
      "2024-03-28 20:27:55,726 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.03207250961347272\n",
      "2024-03-28 20:27:55,726 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.03207250961347272\n",
      "2024-03-28 20:27:55,726 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.03207250961347272\n",
      "2024-03-28 20:27:55,726 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.03207250961347272\n",
      "2024-03-28 20:27:55,726 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.03207250961347272\n",
      "2024-03-28 20:27:55,726 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.03207250961347272\n",
      "2024-03-28 20:27:55,726 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.03207250961347272\n",
      "2024-03-28 20:27:55,726 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.03207250961347272\n",
      "2024-03-28 20:27:55,726 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.03207250961347272\n",
      "2024-03-28 20:27:55,726 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.03207250961347272\n",
      "100% 548/548 [01:40<00:00,  5.47batch/s]\n",
      "2024-03-28 20:29:35,901 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.021469080807083714\n",
      "2024-03-28 20:29:35,901 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.021469080807083714\n",
      "2024-03-28 20:29:35,901 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.021469080807083714\n",
      "2024-03-28 20:29:35,901 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.021469080807083714\n",
      "2024-03-28 20:29:35,901 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.021469080807083714\n",
      "2024-03-28 20:29:35,901 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.021469080807083714\n",
      "2024-03-28 20:29:35,901 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.021469080807083714\n",
      "2024-03-28 20:29:35,901 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.021469080807083714\n",
      "2024-03-28 20:29:35,901 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.021469080807083714\n",
      "2024-03-28 20:29:35,901 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.021469080807083714\n",
      "2024-03-28 20:29:36,058 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:29:36,058 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:29:36,058 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:29:36,058 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:29:36,058 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:29:36,058 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:29:36,058 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:29:36,058 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:29:36,058 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:29:36,058 - INFO - Start Training on Domain 3...\n",
      "100% 549/549 [01:36<00:00,  5.66batch/s]\n",
      "2024-03-28 20:31:13,015 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005628316305265618\n",
      "2024-03-28 20:31:13,015 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005628316305265618\n",
      "2024-03-28 20:31:13,015 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005628316305265618\n",
      "2024-03-28 20:31:13,015 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005628316305265618\n",
      "2024-03-28 20:31:13,015 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005628316305265618\n",
      "2024-03-28 20:31:13,015 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005628316305265618\n",
      "2024-03-28 20:31:13,015 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005628316305265618\n",
      "2024-03-28 20:31:13,015 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005628316305265618\n",
      "2024-03-28 20:31:13,015 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005628316305265618\n",
      "2024-03-28 20:31:13,015 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0005628316305265618\n",
      "100% 549/549 [01:35<00:00,  5.73batch/s]\n",
      "2024-03-28 20:32:48,859 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00330831529657426\n",
      "2024-03-28 20:32:48,859 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00330831529657426\n",
      "2024-03-28 20:32:48,859 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00330831529657426\n",
      "2024-03-28 20:32:48,859 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00330831529657426\n",
      "2024-03-28 20:32:48,859 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00330831529657426\n",
      "2024-03-28 20:32:48,859 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00330831529657426\n",
      "2024-03-28 20:32:48,859 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00330831529657426\n",
      "2024-03-28 20:32:48,859 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00330831529657426\n",
      "2024-03-28 20:32:48,859 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00330831529657426\n",
      "2024-03-28 20:32:48,859 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00330831529657426\n",
      "100% 549/549 [03:41<00:00,  2.47batch/s]\n",
      "2024-03-28 20:36:30,799 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0023921556656577403\n",
      "2024-03-28 20:36:30,799 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0023921556656577403\n",
      "2024-03-28 20:36:30,799 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0023921556656577403\n",
      "2024-03-28 20:36:30,799 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0023921556656577403\n",
      "2024-03-28 20:36:30,799 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0023921556656577403\n",
      "2024-03-28 20:36:30,799 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0023921556656577403\n",
      "2024-03-28 20:36:30,799 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0023921556656577403\n",
      "2024-03-28 20:36:30,799 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0023921556656577403\n",
      "2024-03-28 20:36:30,799 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0023921556656577403\n",
      "2024-03-28 20:36:30,799 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0023921556656577403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1321.904506444931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 3592/3592 [01:08<00:00, 52.55batch/s]\n"
     ]
    }
   ],
   "source": [
    "test# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_sse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_tse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/sse_tse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d7586c-f103-4842-b30a-3eb4c5f2db94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 20:38:43,716 - INFO - Start Training on Domain 0...\n",
      "100% 430/430 [01:13<00:00,  5.88batch/s]\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "2024-03-28 20:39:56,796 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.40733458674945977\n",
      "100% 430/430 [04:43<00:00,  1.52batch/s]\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "2024-03-28 20:44:40,227 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.20595245756977779\n",
      "100% 430/430 [01:21<00:00,  5.29batch/s]\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,573 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.13765098769470413\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 20:46:01,702 - INFO - Start Training on Domain 1...\n",
      "100% 513/513 [01:35<00:00,  5.36batch/s]\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "2024-03-28 20:47:37,406 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.04875446944978618\n",
      "100% 513/513 [01:32<00:00,  5.54batch/s]\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "2024-03-28 20:49:10,048 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.025250461090494086\n",
      "100% 513/513 [01:30<00:00,  5.69batch/s]\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,275 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017038881128788225\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 20:50:40,453 - INFO - Start Training on Domain 2...\n",
      "100% 548/548 [01:36<00:00,  5.70batch/s]\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "2024-03-28 20:52:16,548 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.003836717424288827\n",
      "100% 548/548 [01:36<00:00,  5.69batch/s]\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "2024-03-28 20:53:52,865 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0029737508276232727\n",
      "100% 548/548 [04:42<00:00,  1.94batch/s]\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,394 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.00213909007488793\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 20:58:35,556 - INFO - Start Training on Domain 3...\n",
      "100% 549/549 [02:13<00:00,  4.12batch/s]\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "2024-03-28 21:00:48,993 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0003029442577197035\n",
      "100% 549/549 [01:41<00:00,  5.41batch/s]\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "2024-03-28 21:02:30,495 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00025585578595082833\n",
      "100% 549/549 [04:06<00:00,  2.22batch/s]\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n",
      "2024-03-28 21:06:37,309 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.011493328229298574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1673.729516506195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 4660/4660 [02:44<00:00, 28.36batch/s]\n"
     ]
    }
   ],
   "source": [
    "test# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_sse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_nasdaq\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/sse_nasdaq_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c44a5c8-ef40-49f5-9489-41ce0dc9f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:10:44,195 - INFO - Start Training on Domain 0...\n",
      "100% 591/591 [01:39<00:00,  5.95batch/s]\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "2024-03-28 21:12:23,600 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5282680408386182\n",
      "100% 591/591 [02:46<00:00,  3.54batch/s]\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "2024-03-28 21:15:10,419 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.26531730376045315\n",
      "100% 591/591 [02:33<00:00,  3.84batch/s]\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,154 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.17746143269227072\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:17:44,316 - INFO - Start Training on Domain 1...\n",
      "100% 700/700 [02:10<00:00,  5.38batch/s]\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "2024-03-28 21:19:54,416 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.0007096678151616028\n",
      "100% 700/700 [04:03<00:00,  2.88batch/s]\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "2024-03-28 21:23:57,564 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.03904150170606694\n",
      "100% 700/700 [02:10<00:00,  5.35batch/s]\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,532 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.045083058868047025\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:26:08,718 - INFO - Start Training on Domain 2...\n",
      "100% 719/719 [02:10<00:00,  5.52batch/s]\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "2024-03-28 21:28:19,008 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0014646738127744414\n",
      "100% 719/719 [02:07<00:00,  5.64batch/s]\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "2024-03-28 21:30:26,596 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0009193105619240122\n",
      "100% 719/719 [02:08<00:00,  5.60batch/s]\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,103 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.028855122525657723\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:32:35,271 - INFO - Start Training on Domain 3...\n",
      "100% 719/719 [02:10<00:00,  5.51batch/s]\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "2024-03-28 21:34:45,786 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0018650922167823443\n",
      "100% 719/719 [02:09<00:00,  5.56batch/s]\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "2024-03-28 21:36:55,172 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.001036326802479458\n",
      "100% 719/719 [02:08<00:00,  5.59batch/s]\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n",
      "2024-03-28 21:39:03,706 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.0007808472823885287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1699.732830286026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 4660/4660 [01:26<00:00, 54.06batch/s]\n"
     ]
    }
   ],
   "source": [
    "test# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_szse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_nasdaq\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/szse_nasdaq_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dad6cb0f-45b9-4825-8fc7-d09fe139331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 21:41:20,133 - INFO - Start Training on Domain 0...\n",
      "100% 591/591 [01:38<00:00,  6.00batch/s]\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "2024-03-28 21:42:58,729 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.5637367855688138\n",
      "100% 591/591 [01:45<00:00,  5.59batch/s]\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "2024-03-28 21:44:44,416 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.30629651849875117\n",
      "100% 591/591 [01:45<00:00,  5.62batch/s]\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,601 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.20542767991926952\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 21:46:29,875 - INFO - Start Training on Domain 1...\n",
      "100% 700/700 [02:02<00:00,  5.72batch/s]\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "2024-03-28 21:48:32,301 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.12526603192490127\n",
      "100% 700/700 [02:03<00:00,  5.67batch/s]\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "2024-03-28 21:50:35,840 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.0776270596500087\n",
      "100% 700/700 [02:05<00:00,  5.58batch/s]\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,194 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.06279918544504437\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 21:52:41,440 - INFO - Start Training on Domain 2...\n",
      "100% 719/719 [02:08<00:00,  5.60batch/s]\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "2024-03-28 21:54:49,849 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.0009567898248681107\n",
      "100% 719/719 [02:08<00:00,  5.61batch/s]\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "2024-03-28 21:56:57,974 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0005832128019508632\n",
      "100% 719/719 [02:07<00:00,  5.62batch/s]\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:05,963 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0004110857524844746\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 21:59:06,152 - INFO - Start Training on Domain 3...\n",
      "100% 719/719 [02:08<00:00,  5.60batch/s]\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "2024-03-28 22:01:14,594 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 9.107485838410586e-05\n",
      "100% 719/719 [02:07<00:00,  5.62batch/s]\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "2024-03-28 22:03:22,461 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00010434374011151483\n",
      "100% 719/719 [02:09<00:00,  5.56batch/s]\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n",
      "2024-03-28 22:05:31,785 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 8.273790653071362e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1451.8542320728302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1599/1599 [00:30<00:00, 51.71batch/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_szse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_szse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/szse_szse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7af950a3-a034-4901-a861-3ebcdb582b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:06:36,110 - INFO - Start Training on Domain 0...\n",
      "100% 591/591 [01:38<00:00,  6.02batch/s]\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "2024-03-28 22:08:14,317 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.347696821145436\n",
      "100% 591/591 [01:45<00:00,  5.63batch/s]\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "2024-03-28 22:09:59,364 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.1758589029048968\n",
      "100% 591/591 [01:44<00:00,  5.64batch/s]\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,094 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.12068652604678745\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:11:44,200 - INFO - Start Training on Domain 1...\n",
      "100% 700/700 [02:04<00:00,  5.63batch/s]\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "2024-03-28 22:13:48,462 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.07398040754853615\n",
      "100% 700/700 [02:04<00:00,  5.63batch/s]\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "2024-03-28 22:15:52,852 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.05007437607473029\n",
      "100% 700/700 [02:06<00:00,  5.55batch/s]\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:58,894 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.04156972170674375\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:17:59,055 - INFO - Start Training on Domain 2...\n",
      "100% 719/719 [02:09<00:00,  5.54batch/s]\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "2024-03-28 22:20:08,858 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.002263772888147947\n",
      "100% 719/719 [02:08<00:00,  5.60batch/s]\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "2024-03-28 22:22:17,149 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.0012632789781967357\n",
      "100% 719/719 [02:08<00:00,  5.61batch/s]\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,407 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.0008937206281643529\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:24:25,574 - INFO - Start Training on Domain 3...\n",
      "100% 719/719 [02:07<00:00,  5.63batch/s]\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "2024-03-28 22:26:33,336 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 0.0001803922916403732\n",
      "100% 719/719 [02:08<00:00,  5.58batch/s]\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "2024-03-28 22:28:42,218 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 0.00015608312481799278\n",
      "100% 719/719 [02:08<00:00,  5.57batch/s]\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n",
      "2024-03-28 22:30:51,210 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 0.00014214141499017973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1455.2580258846283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1221/1221 [00:22<00:00, 54.26batch/s]\n"
     ]
    }
   ],
   "source": [
    "test# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_szse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_sse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/szse_sse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d89ea8-8e6e-4912-8db5-314ff74cdf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and get embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "2024-03-28 22:31:59,849 - INFO - Start Training on Domain 0...\n",
      "100% 591/591 [01:36<00:00,  6.12batch/s]\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "2024-03-28 22:33:36,451 - INFO - Task_ID: 0\tEpoch: 0\tAverage Training Loss: 0.3415454381701809\n",
      "100% 591/591 [01:43<00:00,  5.71batch/s]\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "2024-03-28 22:35:20,051 - INFO - Task_ID: 0\tEpoch: 1\tAverage Training Loss: 0.17796194970815768\n",
      "100% 591/591 [01:43<00:00,  5.73batch/s]\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,167 - INFO - Task_ID: 0\tEpoch: 2\tAverage Training Loss: 0.11934958437107844\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "2024-03-28 22:37:03,379 - INFO - Start Training on Domain 1...\n",
      "100% 700/700 [02:01<00:00,  5.76batch/s]\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "2024-03-28 22:39:04,881 - INFO - Task_ID: 1\tEpoch: 0\tAverage Training Loss: 0.030557948828541806\n",
      "100% 700/700 [02:02<00:00,  5.73batch/s]\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "2024-03-28 22:41:07,008 - INFO - Task_ID: 1\tEpoch: 1\tAverage Training Loss: 0.021339123320566224\n",
      "100% 700/700 [02:03<00:00,  5.66batch/s]\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,633 - INFO - Task_ID: 1\tEpoch: 2\tAverage Training Loss: 0.017997827712402104\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "2024-03-28 22:43:10,784 - INFO - Start Training on Domain 2...\n",
      "100% 719/719 [02:06<00:00,  5.69batch/s]\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "2024-03-28 22:45:17,109 - INFO - Task_ID: 2\tEpoch: 0\tAverage Training Loss: 0.001225111520329752\n",
      "100% 719/719 [02:05<00:00,  5.72batch/s]\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "2024-03-28 22:47:22,714 - INFO - Task_ID: 2\tEpoch: 1\tAverage Training Loss: 0.000793646350088471\n",
      "100% 719/719 [02:04<00:00,  5.79batch/s]\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:26,920 - INFO - Task_ID: 2\tEpoch: 2\tAverage Training Loss: 0.000625998698827786\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "2024-03-28 22:49:27,083 - INFO - Start Training on Domain 3...\n",
      "100% 719/719 [02:05<00:00,  5.75batch/s]\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "2024-03-28 22:51:32,191 - INFO - Task_ID: 3\tEpoch: 0\tAverage Training Loss: 6.784585476833524e-05\n",
      "100% 719/719 [02:06<00:00,  5.67batch/s]\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "2024-03-28 22:53:39,047 - INFO - Task_ID: 3\tEpoch: 1\tAverage Training Loss: 8.215668179892697e-05\n",
      "100% 719/719 [02:06<00:00,  5.67batch/s]\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n",
      "2024-03-28 22:55:45,917 - INFO - Task_ID: 3\tEpoch: 2\tAverage Training Loss: 7.229257507986309e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1426.2113330364227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 3592/3592 [01:05<00:00, 55.16batch/s]\n"
     ]
    }
   ],
   "source": [
    "test# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models.Simstock import model\n",
    "from utils.helper import make_noise\n",
    "from utils.prepro import dataset_for_modeling\n",
    "from exp.training import train, test, test_only_inference\n",
    "\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def log(str): logger.info(str)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimStock\")\n",
    "device ='cpu'\n",
    "#'cpu'#\n",
    "datasets = ['train_nasdaq', 'train_sse', 'train_szse', 'train_tse']\n",
    "test_datasets = ['test_before_nasdaq', 'tset_before_sse', 'test_before_szse', 'test_before_tse']\n",
    "\n",
    "# dataset param\n",
    "parser.add_argument(\"--train_dataset\", default=\"train_2022_szse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(datasets))))\n",
    "parser.add_argument(\"--test_dataset\", default=\"test_2022_tse\", type=str, help=\"one of: {}\".format(\", \".join(sorted(test_datasets))))\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int,      help=\"the number of epoches for each task.\")\n",
    "parser.add_argument(\"--data_size\", default=25, type=int,      help=\"the number of input features.\")\n",
    "\n",
    "# model param\n",
    "parser.add_argument(\"--noise_dim\", default=25, type=float,     help=\"the dimension of the LSTM input noise.\")\n",
    "parser.add_argument(\"--latent_dim\", default=25, type=float,     help=\"the latent dimension of RNN variables.\")\n",
    "parser.add_argument(\"--hidden_dim\", default=128, type=float,     help=\"the latent dimension of RNN variables.\") #128\n",
    "parser.add_argument(\"--noise_type\", choices=[\"Gaussian\", \"Uniform\"], default=\"Gaussian\", help=\"The noise type to feed into the generator.\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=1, type=float,   help=\"the number of RNN hierarchical layers.\")\n",
    "parser.add_argument(\"--sector_size\", default=138, type=int,help=\"the number of sector size. WARNING : total + 1\")\n",
    "parser.add_argument(\"--sector_emb\", default=256, type=int,help=\"the number of sector embedding size\")\n",
    "parser.add_argument(\"--lambda_values\", default=0.7, type=float,help=\"the number of sector argument\")\n",
    "\n",
    "# training param\n",
    "parser.add_argument(\"--learning_rate\", default=1e-3, type=float,help=\"the unified learning rate for each single task.\")\n",
    "parser.add_argument(\"--epoches\", default=3, type=int, help=\"the number of epoches for each task.\") # default 3\n",
    "parser.add_argument(\"--save_name\", default=\"test\", type=str,help=\"model save weight\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def main(arsgs):\n",
    "    train_out = dataset_for_modeling(args, train_type = False)\n",
    "    test_out = dataset_for_modeling(args, train_type = True)\n",
    "    \n",
    "    models =  model(args, device).to(device)\n",
    "    optimizer = torch.optim.Adam(models.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    starting_time = time.time()\n",
    "\n",
    "\n",
    "    Es, hiddens = [None], [None]\n",
    "    for task_id, dataloader in enumerate(train_out):\n",
    "        E, hidden, rnn_unit = train(dataloader, optimizer, models, args, log, device, Es[-1], hiddens[-1], task_id)\n",
    "        Es.append(E)\n",
    "        hiddens.append(hidden)    \n",
    "    ending_time = time.time()\n",
    "\n",
    "    print(\"Training time:\", ending_time - starting_time)\n",
    "\n",
    "    # Testing \n",
    "    representation_ll = test(test_out, models, args, log, device, Es[-1], hiddens[-1], is_repre = True) # ~ May 31, 2022 \n",
    "    \n",
    "    return representation_ll\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training and get embeddings\")\n",
    "    rll = main(args)\n",
    "        # For test data\n",
    "    test = pd.read_csv(\"./data_ex_fund/{}.csv\".format(args.test_dataset))\n",
    "\n",
    "    df_embedding = pd.DataFrame(np.concatenate([rll[i].reshape(-1, 1).detach().cpu().numpy() for i in range(len(rll))], axis=0))\n",
    "    test = test.reset_index(drop = True)[[\"Date\",\"Close\",\"Stock_\", \"IndustryCode_\"]]\n",
    "    out_data = pd.concat([test, pd.DataFrame({\"Label\": df_embedding.mean(1).values})], axis = 1)\n",
    "    out_data.to_csv(\"./main_result_ex_fund/szse_tse_2022.csv\", index = False)\n",
    "    out_data = 0\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5d552-9042-473e-a4fe-dedb3cef1c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ce529-6bb1-4f91-a94a-d89277cce2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd4928-8498-44f1-b62e-0776588714b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c97f94-7958-4769-b62e-fd130c33329c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd007dd-d987-4fde-8639-c8bbbc2c8339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048cce4-c41d-47c9-9eec-dc9e16e9d126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d63cfa-265a-4f47-a4ea-dd63f8444112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d83a0-aa99-49da-9eba-c5c7eb5ef474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f9b12-4e35-44f5-a53b-172ed6e36831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigcon",
   "language": "python",
   "name": "bigcon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
